#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞ –∞–Ω–æ–º–∞–ª–∏–π –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–∞—Ö —Å –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è–º–∏ —Ä—É—Å—Å–∫–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã.
–ò—â–µ—Ç —Ç–æ–∫–µ–Ω—ã, –ª–∏—à–Ω–∏–µ –Ω–µ–ø–æ–Ω—è—Ç–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã (–Ω–µ –Ω–∞–∑–≤–∞–Ω–∏—è –≥–ª–∞–≤), –∏ –¥—Ä—É–≥–∏–µ –∞–Ω–æ–º–∞–ª–∏–∏.
"""

import os
import re
import sys
from pathlib import Path

# –ö–∏—Ä–∏–ª–ª–∏—Ü–∞, —Ü–∏—Ñ—Ä—ã, –æ—Å–Ω–æ–≤–Ω—ã–µ –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è, –ø—Ä–æ–±–µ–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
RUSSIAN_CHARS = set('–ê–ë–í–ì–î–ï–Å–ñ–ó–ò–ô–ö–õ–ú–ù–û–ü–†–°–¢–£–§–•–¶–¶–ß–®–©–™–´–¨–≠–Æ–Ø–∞–±–≤–≥–¥–µ—ë–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—ä—ã—å—ç—é—è0123456789')
PUNCTUATION = set('.,;:!?-"()[]{}¬´¬ª‚Äî‚Äì‚Ä¶')
WHITESPACE = set(' \n\r\t')
ALLOWED_CHARS = RUSSIAN_CHARS | PUNCTUATION | WHITESPACE

# –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –Ω–∞–∑–≤–∞–Ω–∏–π –≥–ª–∞–≤ (—Ä–∏–º—Å–∫–∏–µ —Ü–∏—Ñ—Ä—ã, —Å–ª–æ–≤–∞ –ì–õ–ê–í–ê, –ß–ê–°–¢–¨ –∏ —Ç.–¥.)
CHAPTER_PATTERNS = [
    r'^[IVXLCDM]+$',  # –†–∏–º—Å–∫–∏–µ —Ü–∏—Ñ—Ä—ã
    r'^–ì–õ–ê–í–ê\s+[IVXLCDM\d]+',  # –ì–õ–ê–í–ê I, –ì–õ–ê–í–ê 1
    r'^–ß–ê–°–¢–¨\s+[IVXLCDM\d]+',  # –ß–ê–°–¢–¨ I, –ß–ê–°–¢–¨ 1
    r'^[–ì–ß]\s*[IVXLCDM\d]+',  # –ì I, –ß 1
    r'^\d+$',  # –ü—Ä–æ—Å—Ç–æ —á–∏—Å–ª–æ (–º–æ–≥—É—Ç –±—ã—Ç—å –Ω–æ–º–µ—Ä–∞ –≥–ª–∞–≤)
]

def is_likely_chapter_title(line):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Ç—Ä–æ–∫–∞ –Ω–∞–∑–≤–∞–Ω–∏–µ–º –≥–ª–∞–≤—ã"""
    stripped = line.strip()
    if not stripped:
        return False
    
    for pattern in CHAPTER_PATTERNS:
        if re.match(pattern, stripped, re.IGNORECASE):
            return True
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏–µ —Å—Ç—Ä–æ–∫–∏, —Å–æ—Å—Ç–æ—è—â–∏–µ —Ç–æ–ª—å–∫–æ –∏–∑ –∑–∞–≥–ª–∞–≤–Ω—ã—Ö –±—É–∫–≤
    if len(stripped) < 30 and stripped.isupper() and stripped.replace(' ', '').isalpha():
        return True
    
    return False

def is_base64_like(text):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –ø–æ—Ö–æ–∂ –ª–∏ —Ç–µ–∫—Å—Ç –Ω–∞ base64 —Å—Ç—Ä–æ–∫—É"""
    if len(text) < 40:
        return False
    # Base64 —Å–æ–¥–µ—Ä–∂–∏—Ç A-Z, a-z, 0-9, +, /, =
    base64_chars = set('ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/=')
    text_chars = set(text.strip())
    if text_chars.issubset(base64_chars):
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω –∑–Ω–∞–∫ = –∏–ª–∏ /, –∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –º–Ω–æ–≥–æ —Å–∏–º–≤–æ–ª–æ–≤
        if ('=' in text or '/' in text) and len(text.strip()) > 40:
            return True
    return False

def is_normal_latin_usage(line):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –ª–∞—Ç–∏–Ω–∏—Ü–∞ –Ω–æ—Ä–º–∞–ª—å–Ω—ã–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º (–∏–º–µ–Ω–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ, —Ä–∏–º—Å–∫–∏–µ —Ü–∏—Ñ—Ä—ã)"""
    stripped = line.strip()
    
    # –†–∏–º—Å–∫–∏–µ —Ü–∏—Ñ—Ä—ã (I, II, III, IV, V, X, L, C, D, M)
    if re.match(r'^[IVXLCDM]+$', stripped, re.IGNORECASE):
        return True
    
    # –ö–æ—Ä–æ—Ç–∫–∏–µ –ª–∞—Ç–∏–Ω—Å–∫–∏–µ —Å–ª–æ–≤–∞ –≤ —Å–∫–æ–±–∫–∞—Ö (—á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –∫–∞–∫ –ø–æ–º–µ—Ç–∫–∏)
    if re.match(r'^\([a-zA-Z]{1,4}\)$', stripped):
        return True
    
    # –ò–∑–≤–µ—Å—Ç–Ω—ã–µ —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è –∏ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã
    common_abbrevs = {'ISBN', 'ISBN:', 'http', 'https', 'www', 'com', 'ru', 'org', 'net'}
    words = re.findall(r'\b[A-Za-z]+\b', line)
    if words and all(w.upper() in common_abbrevs or len(w) <= 3 for w in words):
        return True
    
    return False

def find_anomalies_in_line(line, line_num):
    """–ù–∞—Ö–æ–¥–∏—Ç –∞–Ω–æ–º–∞–ª–∏–∏ –≤ —Å—Ç—Ä–æ–∫–µ"""
    anomalies = []
    
    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –∏ –Ω–∞–∑–≤–∞–Ω–∏—è –≥–ª–∞–≤
    if not line.strip() or is_likely_chapter_title(line):
        return anomalies
    
    stripped = line.strip()
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ base64-–ø–æ–¥–æ–±–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏ (—Ç–æ–∫–µ–Ω—ã) - –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
    if is_base64_like(stripped):
        anomalies.append({
            'type': 'token_like',
            'line_num': line_num,
            'line': stripped
        })
        return anomalies  # –ï—Å–ª–∏ —ç—Ç–æ —Ç–æ–∫–µ–Ω, –¥—Ä—É–≥–∏–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–µ –Ω—É–∂–Ω—ã
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
    suspicious_chars = []
    for i, char in enumerate(line):
        if char not in ALLOWED_CHARS:
            suspicious_chars.append((i, char, ord(char)))
    
    if suspicious_chars:
        has_russian = any(c in RUSSIAN_CHARS for c in line)
        
        if has_russian:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –ª–∞—Ç–∏–Ω–∏—Ü—É
            latin_chars = [(i, c) for i, c, _ in suspicious_chars if c.isalpha() and ord(c) < 128]
            if latin_chars and not is_normal_latin_usage(line):
                # –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ –ª–∞—Ç–∏–Ω–∏—Ü—ã –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –º–Ω–æ–≥–æ (–Ω–µ –µ–¥–∏–Ω–∏—á–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã)
                if len(latin_chars) > 3:
                    anomalies.append({
                        'type': 'latin_in_russian',
                        'line_num': line_num,
                        'chars': latin_chars[:10],  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è –≤—ã–≤–æ–¥–∞
                        'line': stripped
                    })
            
            # –î—Ä—É–≥–∏–µ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã (–Ω–µ –ª–∞—Ç–∏–Ω–∏—Ü–∞, –Ω–µ –∫–∏—Ä–∏–ª–ª–∏—Ü–∞)
            other_chars = [(i, c, hex(ord(c))) for i, c, code in suspicious_chars 
                          if not (c.isalpha() and ord(c) < 128)]
            # –ò—Å–∫–ª—é—á–∞–µ–º –∏–∑–≤–µ—Å—Ç–Ω—ã–µ –Ω–æ—Ä–º–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã (—Ç–∏—Ä–µ, –∫–∞–≤—ã—á–∫–∏ —Ä–∞–∑–Ω—ã—Ö –≤–∏–¥–æ–≤)
            normal_symbols = set('‚Äî‚Äì‚Ä¶‚Äπ‚Ä∫‚Äö‚Äû¬´¬ª¬∞‚Ññ¬ß')
            other_chars = [(i, c, hex_val) for i, c, hex_val in other_chars if c not in normal_symbols]
            
            if other_chars:
                anomalies.append({
                    'type': 'unusual_symbols',
                    'line_num': line_num,
                    'chars': other_chars[:10],  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è –≤—ã–≤–æ–¥–∞
                    'line': stripped
                })
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –º–Ω–æ–∂–µ—Å—Ç–≤–æ–º –ø–æ–¥—Ä—è–¥ –∏–¥—É—â–∏—Ö –Ω–µ–æ–±—ã—á–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
    # –ù–æ –∏—Å–∫–ª—é—á–∞–µ–º –∏–∑–≤–µ—Å—Ç–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã (—Ç–∏—Ä–µ, –∫–∞–≤—ã—á–∫–∏)
    unusual_pattern = re.search(r'[^\w\s\u0400-\u04FF.,;:!?\-"()\[\]{}¬´¬ª‚Äî‚Äì‚Ä¶¬∞‚Ññ¬ß]{3,}', line)
    if unusual_pattern:
        anomalies.append({
            'type': 'multiple_unusual_chars',
            'line_num': line_num,
            'line': stripped
        })
    
    return anomalies

def analyze_file(filepath):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ñ–∞–π–ª –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –∞–Ω–æ–º–∞–ª–∏–π"""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            lines = f.readlines()
    except UnicodeDecodeError:
        try:
            with open(filepath, 'r', encoding='cp1251') as f:
                lines = f.readlines()
        except Exception as e:
            return {'error': f'–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª: {e}'}
    except Exception as e:
        return {'error': f'–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞: {e}'}
    
    all_anomalies = []
    seen_lines = set()  # –î–ª—è —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    
    for line_num, line in enumerate(lines, 1):
        anomalies = find_anomalies_in_line(line, line_num)
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã - –µ—Å–ª–∏ –¥–ª—è —ç—Ç–æ–π —Å—Ç—Ä–æ–∫–∏ —É–∂–µ –µ—Å—Ç—å –∞–Ω–æ–º–∞–ª–∏—è, –±–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—É—é
        for anomaly in anomalies:
            line_key = (line_num, anomaly['type'])
            if line_key not in seen_lines:
                seen_lines.add(line_key)
                all_anomalies.append(anomaly)
    
    return {
        'total_lines': len(lines),
        'anomalies': all_anomalies
    }

def main():
    script_dir = Path(__file__).parent
    # –§–∞–π–ª—ã —Å —Ç–µ–∫—Å—Ç–∞–º–∏ –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ –ø–∞–ø–∫–µ texts/ –Ω–∞ —É—Ä–æ–≤–µ–Ω—å –≤—ã—à–µ
    texts_dir = script_dir.parent / 'texts'
    txt_files = list(texts_dir.glob('*.txt'))
    
    if not txt_files:
        print("–ù–µ –Ω–∞–π–¥–µ–Ω–æ .txt —Ñ–∞–π–ª–æ–≤ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏")
        return
    
    print(f"–ù–∞–π–¥–µ–Ω–æ {len(txt_files)} .txt —Ñ–∞–π–ª–æ–≤\n")
    print("=" * 80)
    
    files_with_anomalies = []
    
    for txt_file in sorted(txt_files):
        result = analyze_file(txt_file)
        
        if 'error' in result:
            print(f"\n‚ùå {txt_file.name}")
            print(f"   –û—à–∏–±–∫–∞: {result['error']}")
            continue
        
        anomalies = result['anomalies']
        
        if anomalies:
            files_with_anomalies.append((txt_file.name, anomalies))
            print(f"\nüîç {txt_file.name} ({result['total_lines']} —Å—Ç—Ä–æ–∫)")
            
            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –∞–Ω–æ–º–∞–ª–∏–∏ –ø–æ —Ç–∏–ø–∞–º
            by_type = {}
            for anomaly in anomalies:
                a_type = anomaly['type']
                if a_type not in by_type:
                    by_type[a_type] = []
                by_type[a_type].append(anomaly)
            
            for a_type, type_anomalies in sorted(by_type.items()):
                print(f"\n   –¢–∏–ø –∞–Ω–æ–º–∞–ª–∏–∏: {a_type} ({len(type_anomalies)} —Å–ª—É—á–∞–µ–≤)")
                
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3 –ø—Ä–∏–º–µ—Ä–∞ –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞ —Å –ø–æ–ª–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º
                for anomaly in type_anomalies[:3]:
                    line_text = anomaly['line']
                    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –≤—ã–≤–æ–¥ –¥–æ 500 —Å–∏–º–≤–æ–ª–æ–≤, –Ω–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –±–æ–ª—å—à–µ
                    if len(line_text) > 500:
                        print(f"      –°—Ç—Ä–æ–∫–∞ {anomaly['line_num']}: {line_text[:500]}...")
                    else:
                        print(f"      –°—Ç—Ä–æ–∫–∞ {anomaly['line_num']}: {line_text}")
                
                if len(type_anomalies) > 3:
                    print(f"      ... –∏ –µ—â–µ {len(type_anomalies) - 3} —Å–ª—É—á–∞–µ–≤")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –¥–ª—è —Ñ–∞–π–ª–æ–≤ –±–µ–∑ –∞–Ω–æ–º–∞–ª–∏–π
        # (—Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ, –µ—Å–ª–∏ —Ö–æ—Ç–∏—Ç–µ –≤–∏–¥–µ—Ç—å –≤—Å–µ —Ñ–∞–π–ª—ã)
        # else:
        #     print(f"‚úì {txt_file.name} - –∞–Ω–æ–º–∞–ª–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
    
    print("\n" + "=" * 80)
    print(f"\n–ò—Ç–æ–≥–æ: –Ω–∞–π–¥–µ–Ω–æ {len(files_with_anomalies)} —Ñ–∞–π–ª–æ–≤ —Å –∞–Ω–æ–º–∞–ª–∏—è–º–∏ –∏–∑ {len(txt_files)} –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –≤ —Ñ–∞–π–ª (–≤ –ø–∞–ø–∫—É scripts/)
    report_file = script_dir / 'anomalies_report.txt'
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("–û–¢–ß–ï–¢ –û–ë –ê–ù–û–ú–ê–õ–ò–Ø–• –í –¢–ï–ö–°–¢–û–í–´–• –§–ê–ô–õ–ê–•\n")
        f.write("=" * 80 + "\n\n")
        
        for filename, anomalies in files_with_anomalies:
            f.write(f"\n{'='*80}\n")
            f.write(f"–§–∞–π–ª: {filename}\n")
            f.write(f"–í—Å–µ–≥–æ –∞–Ω–æ–º–∞–ª–∏–π: {len(anomalies)}\n")
            f.write(f"{'='*80}\n\n")
            
            by_type = {}
            for anomaly in anomalies:
                a_type = anomaly['type']
                if a_type not in by_type:
                    by_type[a_type] = []
                by_type[a_type].append(anomaly)
            
            for a_type, type_anomalies in sorted(by_type.items()):
                f.write(f"\n–¢–∏–ø: {a_type} ({len(type_anomalies)} —Å–ª—É—á–∞–µ–≤)\n")
                f.write("-" * 80 + "\n")
                
                for anomaly in type_anomalies:
                    f.write(f"\n–°—Ç—Ä–æ–∫–∞ {anomaly['line_num']}:\n")
                    f.write(f"{anomaly['line']}\n")
                    
                    if 'chars' in anomaly:
                        f.write(f"–ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã: {anomaly['chars']}\n")
    
    print(f"\n–î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {report_file}")

if __name__ == '__main__':
    main()
